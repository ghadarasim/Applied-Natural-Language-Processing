{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXTZiUkPpU8i"
   },
   "source": [
    "#Applied Natural Language Processing 955G5\n",
    "##Computer Based Examination, 2023\n",
    "\n",
    "Remember, you can add cells and change their type (between code and text/markdown) as required to answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sHphG0ppUR7"
   },
   "outputs": [],
   "source": [
    "# update your candidate number here\n",
    "candidate_number = 251038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dyg8LujTcyR_"
   },
   "source": [
    "# Question 1 (50 marks)\n",
    "\n",
    "This question is about document similarity and information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J1cyZg-ecD0g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ghadah Al-\n",
      "[nltk_data]     Chaab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ghadah Al-\n",
      "[nltk_data]     Chaab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### do not change the code in this cell\n",
    "# make sure you run this cell\n",
    "from math import log, sqrt\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "documents=[[\"NLTK\", \",\", \"or\", \"Natural\", \"Language\", \"Toolkit\" \",\", \"is\", \"a\", \"Python\", \"package\", \"that\", \"you\", \"can\", \"use\", \"for\", \"NLP\", \".\"],\n",
    "           [\"Neuro-linguistic\", \"programming\", \"(\", \"NLP\", \")\", \"is\", \"a\", \"pseudoscientific\", \"approach\", \"to\", \"communication\" \",\", \"personal\", \"development\", \"and\", \"psychotherapy\", \".\"],\n",
    "           [\"Python\", \"is\", \"a\", \"genus\", \"of\", \"constricting\", \"snakes\", \"in\", \"the\", \"Pythonidae\", \"family\", \".\"]]\n",
    "\n",
    "\n",
    "query=\"NLP Python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7O0BhgAcnqR"
   },
   "source": [
    "a) This question deals with the similarity of documents in terms of comparing the words they contain. Describe two approaches to measuring the similarity of individual words. (4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hROzVWsZeg8w"
   },
   "source": [
    "There are two methods that are frequently used to measuring the similarity of individual words:\n",
    "\n",
    "Cosine Similarity: This approach represents words as vectors in a high-dimensional space, where each dimension corresponds to a word in the vocabulary. The similarity is calculated based on the cosine of the angle between the two word vectors. It takes into account the frequency or occurrence of words in a document. Cosine similarity ranges from 0 to 1, with 1 indicating perfect similarity. However, in the case of individual words, cosine similarity may not be as informative as it is for comparing documents or larger text segments.\n",
    "\n",
    "\n",
    "Similar to Jaccard, this method interprets words as collections of symbols or tokens. By dividing the size of the intersection of the two sets (common characters or tokens) by the size of their union, it determines how similar the two sets are. The Jaccard similarity, for instance, would be 5/6 if we compared the words \"colour\" and \"colour,\" since they share all characters bar the \"u\" in \"colour.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYq7qiLdOrnz"
   },
   "source": [
    "b)\n",
    "i) Write a function that takes a set of tokenized documents as input and returns the same documents after removing stopwords and punctuation, and applying case normalisation to the remaining tokens. So for example, the documents `[[\"This\", \"is\", \"Alice\", \"'s\", \"document\", \".\"], [\"Bob\", \"'s\", \"document\", \".\"]]` would become `[[\"alice\", \"document\"], [\"bob\", \"document\"]]`. Apply this function to the list `documents` to produce a list called `normalised_documents`. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nltk', 'natural', 'language', 'toolkit', 'python', 'package', 'use', 'nlp'], ['neuro-linguistic', 'programming', 'nlp', 'pseudoscientific', 'approach', 'communication', 'personal', 'development', 'psychotherapy'], ['python', 'genus', 'constricting', 'snakes', 'pythonidae', 'family']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ghadah Al-\n",
      "[nltk_data]     Chaab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def preprocessing(text):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    \n",
    "    def preprocess(word):\n",
    "        word = re.sub(r\"(\\w)'s\", r\"\\1\", word)  # to remove possessive 's\n",
    "        word = word.lower().strip(string.punctuation)\n",
    "        if word not in stops and word != \"\":\n",
    "            return word\n",
    "        return None\n",
    "    \n",
    "    return [[preprocess(word) for word in t if preprocess(word) is not None] for t in text]\n",
    "\n",
    "tokenized_documents = documents\n",
    "normalised_documents = preprocessing(tokenized_documents)\n",
    "\n",
    "print(normalised_documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKGAf0XhUvRZ"
   },
   "source": [
    "ii) What impact will stopword removal and case normalisation have on the similarity of documents? (4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPqqwMugVB8K"
   },
   "source": [
    "Stopword removal: Stopwords, such as \"the,\" \"is,\" \"and,\" etc., are often used words in a language that don't have much significance. Eliminating stopwords from a manuscript can assist readers focus on the most crucial words and lessen background noise.By removing stopwords, the similarity of documents may be affected because the frequency and distribution of stopwords can differ between documents. Removing stopwords can also help to highlight the unique content words that contribute to the overall meaning of the document.\n",
    "\n",
    "Case normalisation involves changing all words to either lowercase letters or uppercase letters. By using this method, the effect of letter case on word similarity may be reduced. After case normalisation, \"apple\" and \"Apple,\" for instance, would be considered to be the same word. Case normalization ensures that words with the same letters but different cases are viewed as equivalent, which increases the similarity of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZxs4gW-lQqz"
   },
   "source": [
    "iii) Write a function that takes a list of tokenized documents as input and produces a list of bag-of-words representations. That is, each document should be represented as a FreqDist dictionary mapping tokens to the count of each token in the document. For example, the document `[\"alice\", \"document\"]` would become `FreqDist({'alice': 1, 'document':1})`. Apply this function to the list `normalised_documents` to produce a list called `bags`. (6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk: 1, natural: 1, language: 1, toolkit: 1, python: 1, package: 1, use: 1, nlp: 1, neuro-linguistic: 1, programming: 1, nlp: 1, pseudoscientific: 1, approach: 1, communication: 1, personal: 1, development: 1, psychotherapy: 1, python: 1, genus: 1, constricting: 1, snakes: 1, pythonidae: 1, family: 1, \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Create bags of words\n",
    "def bag_of_words(documents):\n",
    "    bags = [FreqDist(doc) for doc in documents]\n",
    "    return bags\n",
    "\n",
    "bags = bag_of_words(normalised_documents)\n",
    "\n",
    "# Print word frequencies for each document's bag-of-words representation\n",
    "for bag in bags:\n",
    "    for word, frequency in bag.items():\n",
    "        print(f\"{word}: {frequency}\", end=\", \")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2tM7oS-nxzU"
   },
   "source": [
    "iv) Given the definition below, write a function that takes a list of bag-of-words document representations as input and calculates inverse document frequencies. The output of the function should be dictionary that has tokens as keys and inverse document frequencies as values. \n",
    "\n",
    "$$ idf(t) = \\log_2 \\left( \\frac{N}{df(t)} \\right) $$\n",
    "\n",
    "Here, $df(t)$, is the document frequency of the token, $t$, i.e. the number of documents it occurs in, and $N$ is the total number of documents. For example, if alice occurs in 1 out of 2 documents, then the inverse document frequency dictionary should return the value $1 = \\log_2(2)$ for the key `'alice'`.\n",
    "\n",
    "Apply this function to the list `bags` to produce a dictionary called `idf`.\n",
    "\n",
    "(4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EKW-BWQyhL4X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nltk': 1.584962500721156, 'natural': 1.584962500721156, 'language': 1.584962500721156, 'toolkit': 1.584962500721156, 'python': 0.5849625007211562, 'package': 1.584962500721156, 'use': 1.584962500721156, 'nlp': 0.5849625007211562, 'neuro-linguistic': 1.584962500721156, 'programming': 1.584962500721156, 'pseudoscientific': 1.584962500721156, 'approach': 1.584962500721156, 'communication': 1.584962500721156, 'personal': 1.584962500721156, 'development': 1.584962500721156, 'psychotherapy': 1.584962500721156, 'genus': 1.584962500721156, 'constricting': 1.584962500721156, 'snakes': 1.584962500721156, 'pythonidae': 1.584962500721156, 'family': 1.584962500721156}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_idf(bags):\n",
    "    N = len(bags)\n",
    "    df = {}\n",
    "    \n",
    "    # find the frequency for each token\n",
    "    for bag in bags:\n",
    "        for token in bag.keys():\n",
    "            df[token] = df.get(token, 0) + 1\n",
    "    \n",
    "    idf = {token: math.log2(N / df_token) for token, df_token in df.items()}\n",
    "    return idf\n",
    "\n",
    "# apply the function to the list of bags\n",
    "idf = calculate_idf(bags)\n",
    "\n",
    "# Print the inverse document frequency dictionary\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiMtVFZUpy4m"
   },
   "source": [
    "v) Write a function that takes a list of bag-of-words document representations and a dictionary of inverse document frequencies and produces a new set of $tfidf$ representations. Apply this function to the list `bags` and the dictionary `idf` to produce a list of document representations called `tfidf_reps`. (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6ZhX6bXlqpdz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: ['nltk', 'natural', 'language', 'toolkit', 'python', 'package', 'use', 'nlp'] => TF-IDF Representation: {'nltk': 1.584962500721156, 'natural': 1.584962500721156, 'language': 1.584962500721156, 'toolkit': 1.584962500721156, 'python': 0.5849625007211562, 'package': 1.584962500721156, 'use': 1.584962500721156, 'nlp': 0.5849625007211562}\n",
      "Document: ['neuro-linguistic', 'programming', 'nlp', 'pseudoscientific', 'approach', 'communication', 'personal', 'development', 'psychotherapy'] => TF-IDF Representation: {'neuro-linguistic': 1.584962500721156, 'programming': 1.584962500721156, 'nlp': 0.5849625007211562, 'pseudoscientific': 1.584962500721156, 'approach': 1.584962500721156, 'communication': 1.584962500721156, 'personal': 1.584962500721156, 'development': 1.584962500721156, 'psychotherapy': 1.584962500721156}\n",
      "Document: ['python', 'genus', 'constricting', 'snakes', 'pythonidae', 'family'] => TF-IDF Representation: {'python': 0.5849625007211562, 'genus': 1.584962500721156, 'constricting': 1.584962500721156, 'snakes': 1.584962500721156, 'pythonidae': 1.584962500721156, 'family': 1.584962500721156}\n"
     ]
    }
   ],
   "source": [
    "def calculate_tfidf(bags, idf_dict):\n",
    "    tfidf_reps = []\n",
    "    \n",
    "    for bag in bags:\n",
    "        tfidf_rep = {token: tf * idf_dict[token] for token, tf in bag.items()}\n",
    "        tfidf_reps.append(tfidf_rep)\n",
    "    \n",
    "    return tfidf_reps\n",
    "\n",
    "# Apply the function to the list of bags and the idf dictionary\n",
    "tfidf_reps = calculate_tfidf(bags, idf)\n",
    "\n",
    "# Print the TF-IDF representations\n",
    "for doc, tfidf_rep in zip(normalised_documents, tfidf_reps):\n",
    "    print(f\"Document: {doc} => TF-IDF Representation: {tfidf_rep}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KT7us83rXdJ"
   },
   "source": [
    "c)\n",
    "i) Write a function that takes a pair of bag-of-word representations and calculates their dot product. (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product between Document 1 and Document 2: 1\n",
      "Dot Product between Document 1 and Document 3: 1\n",
      "Dot Product between Document 2 and Document 3: 0\n"
     ]
    }
   ],
   "source": [
    "def dot_product(bag1, bag2):\n",
    "    dot_product = sum(bag1.get(token, 0) * bag2.get(token, 0) for token in set(bag1) & set(bag2))\n",
    "    return dot_product\n",
    "# Calculate and print dot products for each pair of bag-of-words representations\n",
    "for i in range(len(bags)):\n",
    "    for j in range(i + 1, len(bags)):\n",
    "        dotproduct = dot_product(bags[i], bags[j])\n",
    "        print(f\"Dot Product between Document {i + 1} and Document {j + 1}: {dotproduct}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsKt6dd1z8pj"
   },
   "source": [
    "ii) Write a function that takes a pair of vectors and calculates their cosine similarity. (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6T2bWTzM0F87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between Document 1 and Document 2: 0.1178511301977579\n",
      "Cosine Similarity between Document 1 and Document 3: 0.14433756729740646\n",
      "Cosine Similarity between Document 2 and Document 3: 0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(vector1.get(token, 0) * vector2.get(token, 0) for token in set(vector1) & set(vector2))\n",
    "    magnitude1 = math.sqrt(sum(val ** 2 for val in vector1.values()))\n",
    "    magnitude2 = math.sqrt(sum(val ** 2 for val in vector2.values()))\n",
    "    \n",
    "    if magnitude1 != 0 and magnitude2 != 0:\n",
    "        cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "        return cosine_similarity\n",
    "    else:\n",
    "        return 0.0  # If one of the vectors is the zero vector, similarity is 0\n",
    "# Calculate and print cosine similarities for each pair of bag-of-words representations\n",
    "for i in range(len(bags)):\n",
    "    for j in range(i + 1, len(bags)):\n",
    "        cosine_similarity = calculate_cosine_similarity(bags[i], bags[j])\n",
    "        print(f\"Cosine Similarity between Document {i + 1} and Document {j + 1}: {cosine_similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqDB8B_wcPBX"
   },
   "source": [
    "iii) Put the string in `query` through the appropriate pre-processing to produce a tfidf bag-of-words representation, and find the index of the most similar document. Print out the corresponding document from the list `documents`. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar Document:\n",
      "['NLTK', ',', 'or', 'Natural', 'Language', 'Toolkit', ',', 'is', 'a', 'Python', 'package', 'that', 'you', 'can', 'use', 'for', 'NLP', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ghadah Al-\n",
      "[nltk_data]     Chaab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ghadah Al-\n",
      "[nltk_data]     Chaab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import math\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocessing(text):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    \n",
    "    def preprocess(word):\n",
    "        word = re.sub(r\"(\\w)'s\", r\"\\1\", word)  # to remove possessive 's\n",
    "        word = word.lower().strip(string.punctuation)\n",
    "        if word not in stops and word != \"\":\n",
    "            return word\n",
    "        return None\n",
    "    \n",
    "    return [[preprocess(word) for word in t if preprocess(word) is not None] for t in text]\n",
    "\n",
    "# Tokenized documents\n",
    "documents = [\n",
    "    [\"NLTK\", \",\", \"or\", \"Natural\", \"Language\", \"Toolkit\", \",\", \"is\", \"a\", \"Python\", \"package\", \"that\", \"you\", \"can\", \"use\", \"for\", \"NLP\", \".\"],\n",
    "    [\"Neuro-linguistic\", \"programming\", \"(\", \"NLP\", \")\", \"is\", \"a\", \"pseudoscientific\", \"approach\", \"to\", \"communication\", \",\", \"personal\", \"development\", \"and\", \"psychotherapy\", \".\"],\n",
    "    [\"Python\", \"is\", \"a\", \"genus\", \"of\", \"constricting\", \"snakes\", \"in\", \"the\", \"Pythonidae\", \"family\", \".\"]\n",
    "]\n",
    "\n",
    "# Apply preprocessing to the documents\n",
    "tokenized_documents = documents\n",
    "normalised_documents = preprocessing(tokenized_documents)\n",
    "\n",
    "# Calculate bags of words from the normalized documents\n",
    "bags = [FreqDist(doc) for doc in normalised_documents]\n",
    "\n",
    "# Query string\n",
    "query = \"NLTK is a Python package for NLP.\"\n",
    "\n",
    "# Preprocess the query string and convert it to a bag-of-words representation\n",
    "query_words = nltk.word_tokenize(query)\n",
    "preprocessed_query = [preprocess(word) for word in query_words if preprocess(word) is not None]\n",
    "query_bag = FreqDist(preprocessed_query)\n",
    "\n",
    "# Calculate TF-IDF representation for the query\n",
    "query_tfidf = {token: tf * idf.get(token, 0) for token, tf in query_bag.items()}\n",
    "\n",
    "# Calculate cosine similarity between query and document bags\n",
    "similarities = [calculate_cosine_similarity(query_tfidf, bag) for bag in bags]\n",
    "\n",
    "# Find the index of the most similar document\n",
    "most_similar_index = similarities.index(max(similarities))\n",
    "\n",
    "# Print the corresponding document\n",
    "print(f\"Most Similar Document:\\n{documents[most_similar_index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
